{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# !pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import datatable as dt\nimport pandas as pd\nimport numpy as np\nimport random\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport psutil\nfrom sklearn.metrics import roc_auc_score\nimport pickle\nfrom collections import defaultdict\nimport json\nfrom time import time\nimport pickle\n\npd.options.mode.chained_assignment = None\n_ = np.seterr(divide='ignore', invalid='ignore')\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_types = {\n    'row_id': 'int32',\n    'timestamp': 'uint64',\n    'user_id': 'uint32',\n    'content_id': 'uint16',\n    'content_type_id': 'int8',\n    'task_container_id': 'uint16',\n    'prior_question_elapsed_time': 'float32',\n    'prior_question_had_explanation': 'boolean'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def jsonKeys2int(x):\n#     return defaultdict(int,{int(k):v for k,v in x.items()})\n\n# with open(\"../input/riiid-attempts-json/attempts.json\",\"r\") as f:\n#     attempts_dict = json.load(f,object_hook=jsonKeys2int)\n\n# attempts_dict = defaultdict(lambda: defaultdict(int),attempts_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jsonKeys2int(x):\n    return {int(k):v for k,v in x.items()}\n\nwith open(\"../input/riiid-attempts-json/attempts_days.json\",\"r\") as f:\n    attempts_dict = json.load(f,object_hook=jsonKeys2int)\nattempts_dict = defaultdict(dict,attempts_dict)\n\n# attempts_dict = defaultdict(dict,{})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/riiid-education/new_questions.pickle', 'rb') as handle:\n    quest_dict = pickle.load(handle)\nwith open('../input/riiid-education/user_dict_pun.pickle', 'rb') as handle:\n    user_dict = pickle.load(handle)\nwith open('../input/riiid-education/model_pun.pickle', 'rb') as handle:\n    lgbm_model = pickle.load(handle)\nwith open('../input/riiid-education/na_fill.pickle', 'rb') as handle:\n    na_fill = pickle.load(handle)\nwith open('../input/riiid-education/train_columns_pun.pickle', 'rb') as handle:\n    train_columns = pickle.load(handle)\nwith open('../input/riiid-education/var_timestamp_day_dict.pickle', 'rb') as handle:\n    user_dict['ans_var_15_day'] = pickle.load(handle)\nwith open('../input/riiid-education/lag_timestamp_dict.pickle','rb') as handle:\n    user_dict['lag_timestamp'] = pickle.load(handle)\nwith open('../input/riiid-education/lecture_in_2.pickle','rb') as handle:\n    user_dict['lecture_in_2'] = pickle.load(handle)\nwith open('../input/riiid-education/part_dict.pickle','rb') as handle:\n    part_dict = pickle.load(handle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"part_dict['part_cumsum'] = defaultdict(int,part_dict['part_cumsum'])\npart_dict['part_cumcount'] = defaultdict(int,part_dict['part_cumcount'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"na_fill['lag_timestamp'] = 25589/(3600*1000)\nna_fill['lag_timestamp2'] = 53351/(3600*1000)\nna_fill['lag_timestamp3'] = 78124/(3600*1000)\nna_fill['lecture_in_2'] = 2\nna_fill['ans_var_15_day'] = 3.8677079787433557e-07\nna_fill['time_diff'] = 3762/1000\nna_fill['prior_question_elapsed_time'] = 21\nna_fill['rows'] = 1\nna_fill['last_attempt_days'] = np.nan\nna_fill['user_time_performance'] = 0\nna_fill['user_elapsed_time'] = 210\nna_fill['part_bundle_id'] = 0\nna_fill['user_punish'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_solved_median  = np.median(list(user_dict['user_qn_solved'].values()))\nqn_solved_median = np.median(list(quest_dict['qn_count'].values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_cols = ['user_qn_solved', 'user_right_ans', 'user_seen_expln', 'user_elapsed_time', 'user_time_performance','user_punish']\nquest_cols = ['qn_part_accuracy', 'qn_tag_accuracy', 'qn_count', 'qn_expln_check', 'qn_median_elapsed_time', 'qn_right_ans','part','rows','part_bundle_id']\nfeat_cols = ['attempts','lag_timestamp','lag_timestamp2','lag_timestamp3','time_diff','lecture_in_2','ans_var_15_day']\npart_cols = ['part_cumsum','part_cumcount']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"def preprocess(val):\n    val = val.astype(data_types)\n    val['timestamp'] = (val['timestamp']/1000).astype('float32')\n    val['prior_question_elapsed_time'] =  (val['prior_question_elapsed_time']/1000).astype('float32')\n  \n    val['prior_question_elapsed_time'] = val['prior_question_elapsed_time'].fillna(na_fill['prior_question_elapsed_time'])\n    val['prior_question_had_explanation'] = val['prior_question_had_explanation'].fillna(False)\n    val['prior_question_had_explanation'] = val['prior_question_had_explanation'].astype('int8')\n    val = val.drop(columns=['prior_group_answers_correct','prior_group_responses'])\n\n    length = val.shape[0]\n\n    user_new_cols = { col: np.full(length,np.nan) for col in user_cols}\n    quest_new_cols = { col: np.full(length,np.nan) for col in quest_cols}\n    feat_new_cols = {col: np.full(length,np.nan) for col in feat_cols}\n    part_new_cols = {col: np.full(length,np.nan) for col in part_cols}\n    \n    for num,row in enumerate(val[['user_id','content_id','prior_question_elapsed_time']].values):\n        for col in user_cols:\n            user_new_cols[col][num] = user_dict[col].get(row[0],na_fill[col])\n        for col in quest_cols:\n            quest_new_cols[col][num] = quest_dict[col].get(row[1],na_fill[col])\n        \n        feat_new_cols['attempts'][num] = attempts_dict[row[0]].get(row[1],np.nan)\n        feat_new_cols['lecture_in_2'][num] = user_dict['lecture_in_2'].get(row[0],na_fill['lecture_in_2'])\n        \n        temp = user_dict['lag_timestamp'].get(row[0],[])\n        if len(temp)>=1:\n            feat_new_cols['lag_timestamp'][num] = user_dict['lag_timestamp'][row[0]][-1]\n            if len(temp)>=2:\n                feat_new_cols['lag_timestamp2'][num] = user_dict['lag_timestamp'][row[0]][-2]\n                feat_new_cols['time_diff'][num] = (user_dict['lag_timestamp'][row[0]][-1] - user_dict['lag_timestamp'][row[0]][-2]) - row[2]*len(user_dict['content_id'].get(row[0],[]))\n                if len(temp)>=3:\n                    feat_new_cols['lag_timestamp3'][num] = user_dict['lag_timestamp'][row[0]][-3]\n        \n        if len(user_dict['ans_var_15_day'].get(row[0],[])) >= 5:\n            feat_new_cols['ans_var_15_day'][num] = np.var(user_dict['ans_var_15_day'][row[0]])\n        \n        \n        part_new_cols['part_cumsum'][num] = part_dict['part_cumsum'][(row[0],quest_dict['part'].get(row[1],1))]\n        part_new_cols['part_cumcount'][num] = part_dict['part_cumcount'][(row[0],quest_dict['part'].get(row[1],1))]\n    \n    \n    \n    val['last_attempt_days'] = (val['timestamp']//(3600*24)) - feat_new_cols['attempts']\n    val['last_attempt_days'] = val['last_attempt_days'].fillna(-1).astype('int16')\n    val['lecture_in_2'] = (feat_new_cols['lecture_in_2']<2).astype('uint8')\n    val['lag_timestamp'] = (val['timestamp'] - feat_new_cols['lag_timestamp'])/3600\n    val['lag_timestamp2'] = (val['timestamp'] - feat_new_cols['lag_timestamp2'])/3600\n    val['lag_timestamp3'] = (val['timestamp'] - feat_new_cols['lag_timestamp3'])/3600\n    val['ans_var_15_day'] = feat_new_cols['ans_var_15_day']\n    val['time_diff'] = feat_new_cols['time_diff']\n    \n    val[['lecture_in_2','lag_timestamp','lag_timestamp2',\n         'lag_timestamp3','ans_var_15_day','time_diff']] = val[['lecture_in_2','lag_timestamp','lag_timestamp2',\n                                                                'lag_timestamp3','ans_var_15_day','time_diff']].fillna(na_fill)\n    \n    val['user_punish'] = user_new_cols['user_punish']/user_new_cols['user_qn_solved']\n    val['user_accuracy'] = user_new_cols['user_right_ans']/user_new_cols['user_qn_solved'] * 10000\n    val['user_mean_seen_expln'] = user_new_cols['user_seen_expln']/user_new_cols['user_qn_solved'] * 10000\n    val['user_mean_elapsed_time'] = user_new_cols['user_elapsed_time']/user_new_cols['user_qn_solved']\n    val['user_mean_time_perf'] = user_new_cols['user_time_performance']/user_new_cols['user_qn_solved']\n    val['user_qn_solved'] = user_new_cols['user_qn_solved'] / user_solved_median\n    \n    val['qn_part_accuracy'] = quest_new_cols['qn_part_accuracy']\n    val['qn_tag_accuracy'] = quest_new_cols['qn_tag_accuracy']\n    val['rows'] = quest_new_cols['rows']\n    \n    val['qn_right_ans'] = quest_new_cols['qn_right_ans'] / quest_new_cols['qn_count']\n    val['qn_median_elapsed_time'] = quest_new_cols['qn_median_elapsed_time'] / quest_new_cols['qn_count']\n    val['qn_expln_check'] = quest_new_cols['qn_expln_check'] / quest_new_cols['qn_count']\n    val['qn_count'] = quest_new_cols['qn_count'] / qn_solved_median\n    val['part'] = quest_new_cols['part']\n    val['part_bundle_id'] = quest_new_cols['part_bundle_id']\n    \n    val['part_cumsum'] = part_new_cols['part_cumsum']\n    val['part_cumcount'] = part_new_cols['part_cumcount']\n    val['part_accuracy'] = (val['part_cumsum']+5*val['qn_part_accuracy'])/(val['part_cumcount']+5)\n    \n#     val = val.fillna(na_fill)\n    val['hmean'] = 2*((val['qn_right_ans']*val['user_accuracy']/10000)/(val['qn_right_ans'] + val['user_accuracy']/10000))\n    \n    val[['user_accuracy',\n         'user_mean_seen_expln']] = np.rint(val[['user_accuracy',\n                                                 'user_mean_seen_expln']]).astype({'user_accuracy':'uint16',\n                                                                                   'user_mean_seen_expln':'uint16'})\n    return val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_from_old(old_data):\n    for user_id, timestamp in old_data.loc[old_data.content_type_id==1,['user_id','timestamp']].drop_duplicates(subset='user_id',keep='first').values:\n        user_dict['lag_timestamp'][user_id] = user_dict['lag_timestamp'].get(user_id,[]) + [timestamp]\n        user_dict['lecture_in_2'][user_id] = 0\n    \n    \n    old_data = old_data.loc[old_data.content_type_id == 0]\n    seen_users = set()\n    for user_id, content_id, answered_correctly, user_qn_solved, qn_count, timestamp, qn_right_ans in old_data[['user_id','content_id','answered_correctly','user_qn_solved','qn_count','timestamp','qn_right_ans']].values:\n\n        if user_qn_solved == 10/user_solved_median and user_id not in seen_users:\n            # This means its a new user\n            user_dict['user_qn_solved'][user_id] = 10\n            user_dict['user_right_ans'][user_id] = na_fill['user_right_ans']\n            user_dict['user_seen_expln'][user_id] = na_fill['user_seen_expln']\n            user_dict['user_elapsed_time'][user_id] = na_fill['user_elapsed_time']\n            user_dict['user_time_performance'][user_id] = na_fill['user_time_performance']\n            user_dict['content_id'][user_id] = set()\n            user_dict['lecture_in_2'][user_id] = 2\n            user_dict['lag_timestamp'][user_id] = []\n            user_dict['ans_var_15_day'][user_id] = [0]\n            user_dict['user_punish'][user_id] = 0\n            \n        if user_id not in seen_users:\n            user_dict['content_id'][user_id] = {content_id}\n            user_dict['lecture_in_2'][user_id] += 1\n            user_dict['lag_timestamp'][user_id].append(timestamp)\n            if len(user_dict['lag_timestamp'][user_id])>3:\n                user_dict['lag_timestamp'][user_id] = user_dict['lag_timestamp'][user_id][-3:]\n            seen_users.add(user_id)\n        else:\n            user_dict['content_id'][user_id].add(content_id)\n        \n        part_dict['part_cumsum'][(user_id,quest_dict['part'][content_id])] += answered_correctly\n        part_dict['part_cumcount'][(user_id,quest_dict['part'][content_id])] += 1\n        \n        attempts_dict[user_id][content_id] = timestamp//(3600*24)\n        \n        user_dict['user_punish'][user_id] += (answered_correctly - qn_right_ans)\n        user_dict['user_qn_solved'][user_id] += 1    \n        user_dict['user_right_ans'][user_id] += answered_correctly\n    \n        quest_dict['qn_count'][content_id] += 1\n        quest_dict['qn_right_ans'][content_id] += answered_correctly\n\n#         attempts_dict[user_id][content_id] = min(attempts_dict[user_id][content_id]+1,4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def update_from_new(new_data):\n    new_data = new_data.loc[new_data.content_type_id == False,['user_id','prior_question_elapsed_time','prior_question_had_explanation','timestamp']]\n    new_data = new_data.dropna(subset=['prior_question_elapsed_time','prior_question_had_explanation'])\n    new_data = new_data.drop_duplicates(subset='user_id',keep='first')\n    \n    new_data['prior_question_elapsed_time'] = np.rint(new_data['prior_question_elapsed_time']/1000).astype('uint16')\n    new_data['prior_question_had_explanation'] = new_data['prior_question_had_explanation'].astype('uint8')\n\n    for user_id, elapsed_time, expln_check, timestamp in new_data[['user_id','prior_question_elapsed_time','prior_question_had_explanation','timestamp']].values:\n        user_dict['ans_var_15_day'][user_id] = user_dict['ans_var_15_day'].get(user_id,[])+[timestamp/(1000*60*60*24)]\n        if len(user_dict['ans_var_15_day'][user_id]) > 15:\n            user_dict['ans_var_15_day'][user_id].pop(0)\n        \n        for content_id in user_dict['content_id'].get(user_id,set()):\n            user_dict['user_elapsed_time'][user_id] += elapsed_time\n            user_dict['user_seen_expln'][user_id] += expln_check\n            \n            quest_dict['qn_median_elapsed_time'][content_id] = quest_dict['qn_median_elapsed_time'].get(content_id,na_fill['qn_median_elapsed_time']) + elapsed_time\n            quest_dict['qn_expln_check'][content_id] = quest_dict['qn_expln_check'].get(content_id,na_fill['qn_expln_check']) + expln_check\n\n            med_time = quest_dict['qn_median_elapsed_time'][content_id]/quest_dict['qn_count'].get(content_id,1)\n            \n#             time_perf = 1\n#             if med_time != 0:\n            time_perf = elapsed_time/med_time\n            \n            time_perf = min(time_perf,2)\n            time_perf = max(time_perf,0.5)\n            time_perf = np.log(time_perf)\n\n            user_dict['user_time_performance'][user_id] += time_perf\n\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SAKT (Only needed for inference part)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import psutil\nimport joblib\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQ = 160","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FFN(nn.Module):\n    def __init__(self, state_size=200):\n        super(FFN, self).__init__()\n        self.state_size = state_size\n\n        self.lr1 = nn.Linear(state_size, state_size)\n        self.relu = nn.ReLU()\n        self.lr2 = nn.Linear(state_size, state_size)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        x = self.lr1(x)\n        x = self.relu(x)\n        x = self.lr2(x)\n        return self.dropout(x)\n\ndef future_mask(seq_length):\n    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n    return torch.from_numpy(future_mask)\n\n\nclass SAKTModel(nn.Module):\n    def __init__(self, n_skill, max_seq=MAX_SEQ, embed_dim=128): #HDKIM 100\n        super(SAKTModel, self).__init__()\n        self.n_skill = n_skill\n        self.embed_dim = embed_dim\n\n        self.embedding = nn.Embedding(2*n_skill+1, embed_dim)\n        self.pos_embedding = nn.Embedding(max_seq-1, embed_dim)\n        self.e_embedding = nn.Embedding(n_skill+1, embed_dim)\n\n        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2)\n\n        self.dropout = nn.Dropout(0.2)\n        self.layer_normal = nn.LayerNorm(embed_dim) \n\n        self.ffn = FFN(embed_dim)\n        self.pred = nn.Linear(embed_dim, 1)\n    \n    def forward(self, x, question_ids):\n        device = x.device        \n        x = self.embedding(x)\n        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n\n        pos_x = self.pos_embedding(pos_id)\n        x = x + pos_x\n\n        e = self.e_embedding(question_ids)\n\n        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n        e = e.permute(1, 0, 2)\n        att_mask = future_mask(x.size(0)).to(device)\n        att_output, att_weight = self.multi_att(e, x, x, attn_mask=att_mask)\n        att_output = self.layer_normal(att_output + e)\n        att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n\n        x = self.ffn(att_output)\n        x = self.layer_normal(x + att_output)\n        x = self.pred(x)\n\n        return x.squeeze(-1), att_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, samples, test_df, skills, max_seq=MAX_SEQ): #HDKIM 100\n        super(TestDataset, self).__init__()\n        self.samples = samples\n        self.user_ids = [x for x in test_df[\"user_id\"].unique()]\n        self.test_df = test_df\n        self.skills = skills\n        self.n_skill = len(skills)\n        self.max_seq = max_seq\n\n    def __len__(self):\n        return self.test_df.shape[0]\n\n    def __getitem__(self, index):\n        test_info = self.test_df.iloc[index]\n\n        user_id = test_info[\"user_id\"]\n        target_id = test_info[\"content_id\"]\n\n        q = np.zeros(self.max_seq, dtype=int)\n        qa = np.zeros(self.max_seq, dtype=int)\n\n        if user_id in self.samples.index:\n            q_, qa_ = self.samples[user_id]\n            \n            seq_len = len(q_)\n\n            if seq_len >= self.max_seq:\n                q = q_[-self.max_seq:]\n                qa = qa_[-self.max_seq:]\n            else:\n                q[-seq_len:] = q_\n                qa[-seq_len:] = qa_          \n        \n        x = np.zeros(self.max_seq-1, dtype=int)\n        x = q[1:].copy()\n        x += (qa[1:] == 1) * self.n_skill\n        \n        questions = np.append(q[2:], [target_id])\n        \n        return x, questions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load SAKT features"},{"metadata":{"trusted":true},"cell_type":"code","source":"skills = joblib.load(\"../input/riiid-sakt-model-dataset-public/skills.pkl.zip\")\nn_skill = len(skills)\ngroup = joblib.load(\"../input/riiid-lgbm-bagging2-sakt-0-781/group.pkl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load SAKT Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = SAKTModel(n_skill, embed_dim=128)\ntry:\n    model.load_state_dict(torch.load(\"../input/sakt-with-randomization-state-updates/SAKT-HDKIM.pt\"))\nexcept:\n    model.load_state_dict(torch.load(\"../input/sakt-with-randomization-state-updates/SAKT-HDKIM.pt\", map_location='cpu'))\nmodel.to(device)\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# user_dict['content_id'] = {i:set() for i in user_dict['content_id'].keys()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example = pd.DataFrame({'row_id':[0,1,2,3,4,5],\n#                        'group_num':[0,0,0,0,0,0],\n#                        'timestamp':[0,10000,10000,25000,25000,50000],\n#                        'user_id':[0,0,0,0,0,0],\n#                        'content_id':[0,1,2,3,4,5],\n#                        'content_type_id':[False,True,True,False,False,False],\n#                        'task_container_id':[0,1,1,2,2,3],\n#                        'prior_question_elapsed_time':[np.nan,10000,10000,5000,5000,3000],\n#                        'prior_question_had_explanation':[np.nan,False,False,True,True,False],\n#                        'prior_group_answers_correct':['[]','[0]','','[1,1]','','[0,1]'],\n#                        'prior_group_responses':[[],[],[],[],[],[]]})\n# separations = example[example.prior_group_answers_correct != \"\"].index.to_list()\n# separations.append(len(example))\n\n# example","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example = dt.fread(\"../input/riiid-test-answer-prediction/example_test.csv\").to_pandas()\n# separations = example[example.prior_group_answers_correct != \"\"].index.to_list()\n# separations.append(len(example))\n# separations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example.loc[1:3,'content_type_id'] = True\n# example.loc[1:3,['prior_question_elapsed_time','prior_question_had_explanation']] = np.nan\n# example.loc[1:3,'content_id'] = [30000,30001,30002]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# res = []\n# ans = []\n# old_data = None\n# prev_df = None\n# for i,j in zip(separations[:4],separations[1:]):\n#     new_data = example.iloc[i:j]\n    \n#     if (old_data is not None):\n#         old_data['answered_correctly'] = eval(new_data['prior_group_answers_correct'].iloc[0])\n#         update_from_old(old_data)\n#         update_from_new(new_data)\n        \n        \n#     if (prev_df is not None) and (psutil.virtual_memory().percent<85):\n#         print(psutil.virtual_memory().percent)\n#         #############################SAKT PART##########################################\n#         prev_df['answered_correctly'] = eval(new_data['prior_group_answers_correct'].iloc[0])\n#         prev_df = prev_df[prev_df['content_type_id'] == False].reset_index(drop=True) \n#         prev_df['prior_question_had_explanation'].fillna(False, inplace=True)       \n#         prev_df.prior_question_had_explanation=prev_df.prior_question_had_explanation.astype('int8')\n#         prev_group = prev_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n#                                                                                             r['content_id'].values,\n#                                                                                             r['answered_correctly'].values))\n\n# #             print(\"I am above update\")\n\n#         for prev_user_id in prev_group.index:\n#             prev_group_content = prev_group[prev_user_id][0]\n#             prev_group_ac = prev_group[prev_user_id][1]\n\n#             if prev_user_id in group.index:\n#                 group[prev_user_id] = (np.append(group[prev_user_id][0],prev_group_content), \n#                                        np.append(group[prev_user_id][1],prev_group_ac))\n#             else:\n#                 group[prev_user_id] = (prev_group_content,prev_group_ac)\n#             if len(group[prev_user_id][0])>MAX_SEQ:\n#                 new_group_content = group[prev_user_id][0][-MAX_SEQ:]\n#                 new_group_ac = group[prev_user_id][1][-MAX_SEQ:]\n#                 group[prev_user_id] = (new_group_content,new_group_ac)\n\n# #             print(\"I am below update\")\n# #         del old_data\n#         ################################################################################\n        \n# #         #############################LGBM PART##########################################\n# #         ### ADD here\n\n# #         \n# #         ################################################################################\n        \n    \n#     #############################SAKT PART##########################################    \n#     prev_df = new_data.copy()\n#     sakt_new_data = new_data.loc[new_data['content_type_id'] == False].reset_index(drop=True)\n    \n#     test_dataset = TestDataset(group, sakt_new_data, skills)\n#     test_dataloader = DataLoader(test_dataset, batch_size=51200, shuffle=False)\n#     SAKT_outs = []\n    \n# #     print(\"I am above second for\")\n    \n#     for item in test_dataloader:\n#         x = item[0].to(device).long()\n#         target_id = item[1].to(device).long()\n\n#         with torch.no_grad():\n#             output, att_weight = model(x, target_id)\n#         SAKT_outs.extend(torch.sigmoid(output)[:, -1].view(-1).data.cpu().numpy())\n   \n# #     print(\"I am below second for\")\n    \n#     ################################################################################\n    \n    \n#     #############################LGBM PART##########################################\n#     new_data = preprocess(new_data)\n#     old_data = new_data.copy()\n#     ################################################################################\n        \n#     new_data['answered_correctly'] = lgbm_model.predict(new_data[train_columns])   \n#     new_data = new_data.loc[new_data['content_type_id'] == 0]\n    \n#     ##### Average of LGBM and SAKT ouput###################\n#     new_data['answered_correctly'] = np.array(SAKT_outs) * 0.5 + np.array(new_data['answered_correctly']) * 0.5\n# #     sakt_new_data['answered_correctly'] = np.array(SAKT_outs)\n    \n#     ans.append(new_data[['row_id', 'answered_correctly']])\n#     res.append(new_data)\n    \n    \n    \n# res = pd.concat(res)\n# ans = pd.concat(ans)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import riiideducation\nimport pandas as pd\nimport numpy as np\n\nenv = riiideducation.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nold_data = None\nprev_df = None\nfor (new_data, sample_prediction_df) in iter_test:\n    if (old_data is not None):\n        old_data['answered_correctly'] = eval(new_data['prior_group_answers_correct'].iloc[0])\n        update_from_old(old_data)\n        update_from_new(new_data)\n        \n        \n    if (prev_df is not None) and (psutil.virtual_memory().percent<90):\n#         print(psutil.virtual_memory().percent)\n        #############################SAKT PART##########################################\n        prev_df['answered_correctly'] = eval(new_data['prior_group_answers_correct'].iloc[0])\n        prev_df = prev_df[prev_df['content_type_id'] == False].reset_index(drop=True) \n        prev_df['prior_question_had_explanation'].fillna(False, inplace=True)       \n        prev_df.prior_question_had_explanation=prev_df.prior_question_had_explanation.astype('int8')\n        prev_group = prev_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n                                                                                            r['content_id'].values,\n                                                                                            r['answered_correctly'].values))\n\n#             print(\"I am above update\")\n\n        for prev_user_id in prev_group.index:\n            prev_group_content = prev_group[prev_user_id][0]\n            prev_group_ac = prev_group[prev_user_id][1]\n\n            if prev_user_id in group.index:\n                group[prev_user_id] = (np.append(group[prev_user_id][0],prev_group_content), \n                                       np.append(group[prev_user_id][1],prev_group_ac))\n            else:\n                group[prev_user_id] = (prev_group_content,prev_group_ac)\n            if len(group[prev_user_id][0])>MAX_SEQ:\n                new_group_content = group[prev_user_id][0][-MAX_SEQ:]\n                new_group_ac = group[prev_user_id][1][-MAX_SEQ:]\n                group[prev_user_id] = (new_group_content,new_group_ac)\n\n#             print(\"I am below update\")\n        ################################################################################\n\n        \n    \n    #############################SAKT PART##########################################    \n    prev_df = new_data.copy()\n    sakt_new_data = new_data.loc[new_data['content_type_id'] == False].reset_index(drop=True)\n    \n    test_dataset = TestDataset(group, sakt_new_data, skills)\n    test_dataloader = DataLoader(test_dataset, batch_size=51200, shuffle=False)\n    SAKT_outs = []\n    \n#     print(\"I am above second for\")\n    \n    for item in test_dataloader:\n        x = item[0].to(device).long()\n        target_id = item[1].to(device).long()\n\n        with torch.no_grad():\n            output, att_weight = model(x, target_id)\n        SAKT_outs.extend(torch.sigmoid(output)[:, -1].view(-1).data.cpu().numpy())\n   \n#     print(\"I am below second for\")\n    \n    ################################################################################\n    \n    \n    #############################LGBM PART##########################################\n    new_data = preprocess(new_data)\n    old_data = new_data.copy()\n    ################################################################################\n        \n    new_data['answered_correctly'] = lgbm_model.predict(new_data[train_columns])   \n    new_data = new_data.loc[new_data['content_type_id'] == 0]\n    \n    ##### Average of LGBM and SAKT ouput###################\n    new_data['answered_correctly'] = np.array(SAKT_outs) * 0.05 + np.array(new_data['answered_correctly']) * 0.95\n    \n    env.predict(new_data[['row_id', 'answered_correctly']])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ns = pd.DataFrame()\ns.to_csv('submission.csv')","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}